fit <- glm(use~magn+wind-1,data=shuttle,family="binomial")
fit <- glm(use~magn+wind-1,data=shuttle,family="binomial")
summary(fit)
str(shuttle)
fit <- glm(use~wind-1,data=shuttle,family="binomial")
summary(fit)
fit <- glm(use~wind,data=shuttle,family="binomial")
summary(fit)
fit <- glm(use~wind+magn,data=shuttle,family="binomial")
summary(fit)
exp(-0.3635)/exp(-0.3635-0.03201)
fit <- glm(use~factor(wind)+factor(magn),data=shuttle,family="binomial")
summary(fit)
fit <- glm(use~factor(wind)+factor(magn)-1,data=shuttle,family="binomial")
summary(fit)
exp(-3.635e-01)/exp(-3.955e-01)
coef(fit)
exp(coef(fit)[1:2])
0.6733312/0.6952323
(Coef2 <- coef(summary(fit)))
coef2.odds <- exp(c(Coef2[1, 1], Coef2[2, 1]))
(odds2.ratio <- coef2.odds[1] / coef2.odds[2]) # "head" is the reference
data("InsectSprays")
?InsectSprays
head(InsectSprays)
fit.2 <- glm(factor(spray)~count,family = "poisson")
fit.2 <- glm(factor(spray)~count,family = "poisson",data=InsectSprays)
fit.2 <- glm(count~factor(spray),family = "poisson",data=InsectSprays)
summary(fit.2)
fit.2 <- glm(factor(spray)~count-1,family = "poisson",data=InsectSprays)
fit.2 <- glm(count~factor(spray)-1,family = "poisson",data=InsectSprays)
summary(fit.2)
2.67415 /2.73003
exp( 2.67415-2.73003)
knots=0
splineTerm <- sapply(knots, function(knot){(x>knot)*(x-knot)})
xMat <- cbind(1, x, splineTerm)
lm(y~xMat-1)
summary(lm(y~xMat-1))
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots=0
splineTerm <- sapply(knots, function(knot){(x>knot)*(x-knot)})
xMat <- cbind(1, x, splineTerm)
lm(y~xMat-1)
plot(x,y,frame=F)
lines(x,yhat)
yhat <- predict(lm(y~xMat-1))
plot(x,y,frame=F)
lines(x,yhat)
xMat
library(knitr)
summary(mtcars)
str(mtcars)
pairs(mtcars)
fit <- lm(mpg~., data=mtcars)
summary(fit)
?step
?stepAIC
stepmodel <- step(fit,direction = "both")
fit.1 <- lm(mpg ~ wt + qsec + am, data=mtcars)
summary(fit.1)
summary(fit.1)
plot(fit.1)
par(mfrow=c(2,2))
plot(fit.1)
vif(fit.1)
library(car)
vif(fit.1)
leverage <- hatvalues(fit.1)
influential <- dfbetas(fit.1)
leverage
influential <- dfbetas(fit.1)
influential
?mtcars
str(mtcars)
mtcars$vs <- factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
summary(mtcars)
str(mtcars)
pairs(mtcars)
barplot(mpg~am, data=mtcars)
?barplot
boxplot(mpg~am, data=mtcars)
boxplot(mpg~am, data=mtcars, xlab="Transmission", ylab="Miles/(US) gallon", legend=c("automatic", "manual"))
boxplot(mpg~am, data=mtcars, xlab="Transmission", ylab="Miles/(US) gallon")
fit <- lm(mpg~am, data=mtcars)
summary(fit)
t.test(mpg~am, data=mtcars)
t.test(mpg~am, data=mtcars)
fit <- lm(mpg~am, data=mtcars)
par(mfrow=c(2,2))
plot(fit)
stepmodel <- step(fit,direction = "both")
stepmodel <- step(lm(mpg~., data = mtcars),direction = "both")
stepmodel <- step(lm(mpg~., data = mtcars),direction = "both")
summary(stepmodel)
par(mfrow=c(2,2))
plot(stepmodel)
stepAIC(fit,direction = "both")
?stepAIC
stepAIC(fit,direction = "forward")
stepmodel <- step(lm(mpg~., data = mtcars),direction = "both", trace = 0)
summary(stepmodel)
anova(fit,stepmodel)
summary(stepmodel)
?mtcars
anova(fit,stepmodel)
tail(sort(leverage),3)
sort(leverage)
tail(sort(influential[,6]),3)
influential
tail(sort(influential),3)
leverage <- hatvalues(stepmodel)
tail(sort(leverage),3)
?dfbetas
tail(sort(influential),3)
summary(stepmodel)
install.packages("shiny")
library(shiny)
source('~/.active-rstudio-document', echo=TRUE)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"), sidebarPanel(
h3('Sidebar text') ),
mainPanel(
h3('Main Panel text')
)
))
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"), sidebarPanel(
h3('Sidebar text') ),
mainPanel(
h3('Main Panel text')
)
))
runApp()
setwd("~/Desktop/Federal-Paper-Authorship-Classification")
#install.packages("tm")
library(tm)
library(SnowballC)
# This code uses tm to preprocess the papers into a format useful for NB
## 1) remove non-letter characters, 2)remove stopwords, and 3)remove stem words
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus( ds )
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory('fp_hamilton_test')
preprocess.directory('fp_hamilton_test')
preprocess.directory('fp_hamilton_train')
preprocess.directory('fp_madison_test')
preprocess.directory('fp_madison_train')
# This code uses tm to preprocess the papers into a format useful for NB
## 1) remove non-letter characters, 2)remove stopwords, and 3)remove stem words
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus( ds )
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
preprocess.directory('fp_hamilton_test')
preprocess.directory('fp_hamilton_train')
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus( ds )
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
preprocess.directory("fp_hamilton_test")
Corpus("fp_hamilton_test")
Corpus(DirSource("fp_hamilton_test"))
preprocess.directory("fp_hamilton_test")
source('hw04.R')
preprocess.directory("fp_hamilton_test")
library(tm)
library(SnowballC)
Corpus(DirSource("fp_hamilton_test"))
mydf <- data.frame(letter_factor = factor(rep(letters[1:3], each = 2)),
some_ints = 1L:6L,
some_text = paste0("This is text number ", 1:6, "."),
stringsAsFactors = FALSE,
row.names = paste0("fromDf_", 1:6))
mydf
corpus(mydf)
Corpus(mydf)
Corpora(DirSource("fp_hamilton_test"))
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
#fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
fp = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
#fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
?tm_map
data("crude")
?data("crude")
fp = Corpus(ds)
ds
Corpora(DirSource("fp_hamilton_test"))
Corpus(DirSource("fp_hamilton_test"))
VCorpus(DirSource("fp_hamilton_test"))
setwd("~/Desktop/Federal-Paper-Authorship-Classification")
file.path("C:", "texts")
file.path("~/Desktop/Federal-Paper-Authorship-Classification", "fp_hamilton_test")
cname=file.path("~/Desktop/Federal-Paper-Authorship-Classification", "fp_hamilton_test")
dir(cname)
docs <- Corpus(DirSource(cname))
summary(docs)
docs
docs[[1]]
tm_map( docs[[1]] , tolower)
tm_map( docs[1] , tolower)
length(docs)
dirname="fp_hamilton_test"
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
i=1
fp[i] = tm_map( fp[i] , tolower);
fp[1]
tm_map( fp[1] , tolower)
tm_map( fp[1] , content_transformer(tolower))
tm_map( fp[i] , removePunctuation);
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
fp[i] = tm_map( fp[i] , tolower);
fp[i] = tm_map( fp[i] , removePunctuation);
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
fp[i] = tm_map( fp[i], stemDocument)
fp[i] = tm_map( fp[i], stripWhitespace)
length(fp)
fp = Corpus(ds)
fp
length(fp)
preprocess.directory("fp_hamilton_test")
preprocess.directory('fp_hamilton_train')
preprocess.directory('fp_madison_test')
preprocess.directory('fp_madison_train')
preprocess.directory("fp_hamilton_test")
preprocess.directory('fp_hamilton_train')
preprocess.directory('fp_madison_test')
preprocess.directory('fp_madison_train')
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
fp[i] = tm_map( fp[i] , removePunctuation);
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
fp[i] = tm_map( fp[i] , removePunctuation);
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
i=3
fp[i] = tm_map( fp[i] , removePunctuation);
fp[3]
tm_map( fp[i] , removePunctuation)
class(fp[i])
class(fp[3])
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
#fp[i] = tm_map( fp[i] , removePunctuation);
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
preprocess.directory('fp_hamilton_train')
preprocess.directory('fp_madison_test')
preprocess.directory('fp_madison_train')
source('hw04.R')
#Problem1 Step1
preprocess.directory("fp_hamilton_test")
library(data.table)
?fread
train<-fread("~Desktop/Facebook/train.csv",header = T)
train<-fread("~/Desktop/Facebook/train.csv",header = T)
head(train)
require(bit64)
install.packages("bit64")
require(bit64)
train<-fread("~/Desktop/Facebook/train.csv",header = T)
head(train)
class(train)
summary(train$accuracy)
train %>% sample_frac(0.01) %>% ggplot(aes(x=accuracy))+geom_density()
library(dplyr)
train %>% sample_frac(0.01) %>% ggplot(aes(x=accuracy))+geom_density()
library(ggplot2)
train %>% sample_frac(0.01) %>% ggplot(aes(x=accuracy))+geom_density()
?sample_frac
?stat_summary_2d
setwd("~/Desktop/Federal-Paper-Authorship-Classification")
#install.packages("tm")
library(tm)
library(SnowballC)
# This code uses tm to preprocess the papers into a format useful for NB
## 1) remove non-letter characters, 2)remove stopwords, and 3)remove stem words
preprocess.directory = function(dirname){
# the directory must have all the relevant text files
ds = DirSource(dirname)
# Corpus will make a tm document corpus from this directory
fp = Corpus(ds)
# inspect to verify
# inspect(fp[1])
# another useful command
# identical(fp[[1]], fp[["Federalist01.txt"]])
# now let us iterate through and clean this up using tm functionality
for (i in 1:length(fp)){
# make all words lower case
fp[i] = tm_map( fp[i] , tolower);
# remove all punctuation
fp[i] = tm_map( fp[i] , removePunctuation);
# remove stopwords like the, a, and so on.
fp[i] = tm_map( fp[i], removeWords, stopwords("english"));
# remove stems like suffixes
fp[i] = tm_map( fp[i], stemDocument)
# remove extra whitespace
fp[i] = tm_map( fp[i], stripWhitespace)
}
# now write the corpus out to the files for our future use.
# MAKE SURE THE _CLEAN DIRECTORY EXISTS
writeCorpus( fp , sprintf('%s_clean',dirname) )
}
##########################################
preprocess.directory("fp_hamilton_test")
preprocess.directory('fp_hamilton_train')
preprocess.directory('fp_madison_test')
preprocess.directory('fp_madison_train')
summary(train$x)
